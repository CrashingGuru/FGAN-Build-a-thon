{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlj+DSfmNs9M5zkNfUHyM+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrra/FGAN-Build-a-thon/blob/main/PythonScripts2023/Spacy-example-01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vya8W1t0bpW7"
      },
      "outputs": [],
      "source": [
        "pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textacy"
      ],
      "metadata": {
        "id": "iKk8SsQLkF2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n"
      ],
      "metadata": {
        "id": "tz0Cg-B5cUa8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy\n"
      ],
      "metadata": {
        "id": "m5sUPAb9mC6i"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy.extract"
      ],
      "metadata": {
        "id": "iEMWZWlzol-_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "A4qRIWKPcZTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")"
      ],
      "metadata": {
        "id": "vD6ZAzQjceZb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n"
      ],
      "metadata": {
        "id": "5GwXmGO7chTi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yiz9SbQBcj9i",
        "outputId": "35e0279c-f0c3-4d83-901d-b594e612d3c3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkD9l30zcopS",
        "outputId": "b71f090d-c6da-4939-e4ec-48408d532bfe"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"All living things are made of cells.\",\n",
        "             \"Aaron is a good boy.\",\n",
        "             \"cells has organelles\",\n",
        "             \"Kennedy is in UK.\"]"
      ],
      "metadata": {
        "id": "aEdsz5v8k72l"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verb_patterns = [[{\"POS\":\"AUX\"}, {\"POS\":\"VERB\"},\n",
        "                  {\"POS\":\"ADP\"}],\n",
        "                 [{\"POS\":\"AUX\"}]]"
      ],
      "metadata": {
        "id": "zjboVK2MiNi7"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def contains_root(verb_phrase, root):\n",
        "    vp_start = verb_phrase.start\n",
        "    vp_end = verb_phrase.end\n",
        "    if (root.i >= vp_start and root.i <= vp_end):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "RKTOWaSciPSR"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_root_of_sentence(doc):\n",
        "    root_token = None\n",
        "    for token in doc:\n",
        "        if (token.dep_ == \"ROOT\"):\n",
        "            root_token = token\n",
        "    return root_token"
      ],
      "metadata": {
        "id": "mbeUBzwglR_k"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_verb_phrases(doc):\n",
        "    root = find_root_of_sentence(doc)\n",
        "    verb_phrases= textacy.extract.matches.token_matches(doc,verb_patterns)\n",
        "\n",
        "    new_vps = []\n",
        "    for verb_phrase in verb_phrases:\n",
        "        if (contains_root(verb_phrase, root)):\n",
        "            new_vps.append(verb_phrase)\n",
        "    return new_vps"
      ],
      "metadata": {
        "id": "y0p4-uESkf8h"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def longer_verb_phrase(verb_phrases):\n",
        "    longest_length = 0\n",
        "    longest_verb_phrase = None\n",
        "    for verb_phrase in verb_phrases:\n",
        "        if len(verb_phrase) > longest_length:\n",
        "            longest_verb_phrase = verb_phrase\n",
        "    return longest_verb_phrase"
      ],
      "metadata": {
        "id": "9CoT6bI4ki7F"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_noun_phrase(verb_phrase, noun_phrases, side):\n",
        "    for noun_phrase in noun_phrases:\n",
        "        if (side == \"left\" and \\\n",
        "            noun_phrase.start < verb_phrase.start):\n",
        "            return noun_phrase\n",
        "        elif (side == \"right\" and \\\n",
        "              noun_phrase.start > verb_phrase.start):\n",
        "            return noun_phrase"
      ],
      "metadata": {
        "id": "F_xfUczSkljV"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_triplet(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    verb_phrases = get_verb_phrases(doc)\n",
        "    noun_phrases = doc.noun_chunks\n",
        "    verb_phrase = None\n",
        "\n",
        "    if (len(verb_phrases) > 1):\n",
        "        verb_phrase = \\\n",
        "        longer_verb_phrase(list(verb_phrases))\n",
        "    elif (len(verb_phrases)==1):\n",
        "        verb_phrase = verb_phrases[0]\n",
        "    elif (len(verb_phrases)==0):\n",
        "        print(\"verb_phrases is NULL\")\n",
        "        return(\"\",\"\",\"\")\n",
        "\n",
        "    left_noun_phrase = find_noun_phrase(verb_phrase,\n",
        "                                        noun_phrases,\n",
        "                                        \"left\")\n",
        "    right_noun_phrase = find_noun_phrase(verb_phrase,\n",
        "                                         noun_phrases,\n",
        "                                         \"right\")\n",
        "    return (left_noun_phrase, verb_phrase,\n",
        "            right_noun_phrase)"
      ],
      "metadata": {
        "id": "HlhTfAW2kpCB"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "    (left_np, vp, right_np) = find_triplet(sentence)\n",
        "    print(\"Triplets = \",left_np, \"\\t\", vp, \"\\t\", right_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NELcGT_-ktna",
        "outputId": "62420c1c-e709-4a3c-a475-ffde1055ab4b"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triplets =  All living things \t are made of \t cells\n",
            "Triplets =  Aaron \t is \t a good boy\n",
            "verb_phrases is NULL\n",
            "Triplets =   \t  \t \n",
            "Triplets =  Kennedy \t is \t UK\n"
          ]
        }
      ]
    }
  ]
}